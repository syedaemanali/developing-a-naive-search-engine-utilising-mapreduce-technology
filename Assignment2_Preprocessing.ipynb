{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Importing Libraries***"
      ],
      "metadata": {
        "id": "DNsuV8n8DFcI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ydm03S2jv-5",
        "outputId": "0c35455c-3fe9-4dcc-fa3a-ee86d5d27453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Reading & Storing Data in Chunks***"
      ],
      "metadata": {
        "id": "t8zH7ynrDPZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_and_store_chunks(input_file, chunk_size=1000):\n",
        "    chunks = []\n",
        "    try:\n",
        "        # Iterate over the CSV file in chunks\n",
        "        for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, error_bad_lines=False)):\n",
        "            # Process each chunk as needed\n",
        "            chunk.dropna(inplace=True)  # Drop rows with missing values\n",
        "            chunk['SECTION_TEXT'] = chunk['SECTION_TEXT'].astype(str).apply(preprocess_text)\n",
        "            chunks.append(chunk)  # Append the processed chunk to the list\n",
        "            print(f\"Processed chunk {i+1}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV: {e}\")\n",
        "    if chunks:\n",
        "        return pd.concat(chunks, ignore_index=True)  # Concatenate all chunks into a single DataFrame\n",
        "    else:\n",
        "        print(\"No valid data chunks found\")\n",
        "        return None\n",
        "\n",
        "# Read CSV file in chunks and store its content in a DataFrame\n",
        "input_data_df = read_csv_and_store_chunks(\"/content/enwiki-20170820.csv\", chunk_size=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh9xmxINCEji",
        "outputId": "b6a58414-86e2-4537-9e30-91de14e91408"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-dd39ff0f6e15>:5: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, error_bad_lines=False)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed chunk 1\n",
            "Processed chunk 2\n",
            "Processed chunk 3\n",
            "Processed chunk 4\n",
            "Processed chunk 5\n",
            "Processed chunk 6\n",
            "Processed chunk 7\n",
            "Processed chunk 8\n",
            "Processed chunk 9\n",
            "Processed chunk 10\n",
            "Processed chunk 11\n",
            "Processed chunk 12\n",
            "Processed chunk 13\n",
            "Processed chunk 14\n",
            "Processed chunk 15\n",
            "Processed chunk 16\n",
            "Processed chunk 17\n",
            "Processed chunk 18\n",
            "Processed chunk 19\n",
            "Processed chunk 20\n",
            "Processed chunk 21\n",
            "Processed chunk 22\n",
            "Processed chunk 23\n",
            "Processed chunk 24\n",
            "Processed chunk 25\n",
            "Processed chunk 26\n",
            "Processed chunk 27\n",
            "Processed chunk 28\n",
            "Processed chunk 29\n",
            "Processed chunk 30\n",
            "Processed chunk 31\n",
            "Processed chunk 32\n",
            "Processed chunk 33\n",
            "Processed chunk 34\n",
            "Processed chunk 35\n",
            "Processed chunk 36\n",
            "Processed chunk 37\n",
            "Processed chunk 38\n",
            "Processed chunk 39\n",
            "Processed chunk 40\n",
            "Processed chunk 41\n",
            "Processed chunk 42\n",
            "Processed chunk 43\n",
            "Processed chunk 44\n",
            "Processed chunk 45\n",
            "Processed chunk 46\n",
            "Processed chunk 47\n",
            "Processed chunk 48\n",
            "Processed chunk 49\n",
            "Processed chunk 50\n",
            "Processed chunk 51\n",
            "Processed chunk 52\n",
            "Processed chunk 53\n",
            "Processed chunk 54\n",
            "Processed chunk 55\n",
            "Processed chunk 56\n",
            "Processed chunk 57\n",
            "Processed chunk 58\n",
            "Processed chunk 59\n",
            "Processed chunk 60\n",
            "Processed chunk 61\n",
            "Processed chunk 62\n",
            "Processed chunk 63\n",
            "Processed chunk 64\n",
            "Processed chunk 65\n",
            "Processed chunk 66\n",
            "Processed chunk 67\n",
            "Processed chunk 68\n",
            "Processed chunk 69\n",
            "Processed chunk 70\n",
            "Processed chunk 71\n",
            "Processed chunk 72\n",
            "Processed chunk 73\n",
            "Processed chunk 74\n",
            "Processed chunk 75\n",
            "Processed chunk 76\n",
            "Processed chunk 77\n",
            "Processed chunk 78\n",
            "Processed chunk 79\n",
            "Processed chunk 80\n",
            "Processed chunk 81\n",
            "Processed chunk 82\n",
            "Processed chunk 83\n",
            "Processed chunk 84\n",
            "Processed chunk 85\n",
            "Processed chunk 86\n",
            "Processed chunk 87\n",
            "Processed chunk 88\n",
            "Processed chunk 89\n",
            "Processed chunk 90\n",
            "Processed chunk 91\n",
            "Processed chunk 92\n",
            "Processed chunk 93\n",
            "Processed chunk 94\n",
            "Processed chunk 95\n",
            "Processed chunk 96\n",
            "Processed chunk 97\n",
            "Processed chunk 98\n",
            "Processed chunk 99\n",
            "Processed chunk 100\n",
            "Processed chunk 101\n",
            "Processed chunk 102\n",
            "Processed chunk 103\n",
            "Processed chunk 104\n",
            "Processed chunk 105\n",
            "Processed chunk 106\n",
            "Processed chunk 107\n",
            "Processed chunk 108\n",
            "Processed chunk 109\n",
            "Processed chunk 110\n",
            "Processed chunk 111\n",
            "Processed chunk 112\n",
            "Processed chunk 113\n",
            "Processed chunk 114\n",
            "Processed chunk 115\n",
            "Processed chunk 116\n",
            "Processed chunk 117\n",
            "Processed chunk 118\n",
            "Processed chunk 119\n",
            "Processed chunk 120\n",
            "Processed chunk 121\n",
            "Processed chunk 122\n",
            "Processed chunk 123\n",
            "Processed chunk 124\n",
            "Processed chunk 125\n",
            "Processed chunk 126\n",
            "Processed chunk 127\n",
            "Processed chunk 128\n",
            "Processed chunk 129\n",
            "Processed chunk 130\n",
            "Processed chunk 131\n",
            "Processed chunk 132\n",
            "Processed chunk 133\n",
            "Processed chunk 134\n",
            "Processed chunk 135\n",
            "Processed chunk 136\n",
            "Processed chunk 137\n",
            "Processed chunk 138\n",
            "Processed chunk 139\n",
            "Processed chunk 140\n",
            "Processed chunk 141\n",
            "Processed chunk 142\n",
            "Processed chunk 143\n",
            "Processed chunk 144\n",
            "Processed chunk 145\n",
            "Processed chunk 146\n",
            "Processed chunk 147\n",
            "Processed chunk 148\n",
            "Processed chunk 149\n",
            "Processed chunk 150\n",
            "Processed chunk 151\n",
            "Processed chunk 152\n",
            "Processed chunk 153\n",
            "Processed chunk 154\n",
            "Processed chunk 155\n",
            "Processed chunk 156\n",
            "Processed chunk 157\n",
            "Processed chunk 158\n",
            "Processed chunk 159\n",
            "Processed chunk 160\n",
            "Processed chunk 161\n",
            "Processed chunk 162\n",
            "Processed chunk 163\n",
            "Processed chunk 164\n",
            "Processed chunk 165\n",
            "Processed chunk 166\n",
            "Processed chunk 167\n",
            "Processed chunk 168\n",
            "Processed chunk 169\n",
            "Processed chunk 170\n",
            "Processed chunk 171\n",
            "Processed chunk 172\n",
            "Processed chunk 173\n",
            "Processed chunk 174\n",
            "Processed chunk 175\n",
            "Processed chunk 176\n",
            "Processed chunk 177\n",
            "Processed chunk 178\n",
            "Processed chunk 179\n",
            "Processed chunk 180\n",
            "Processed chunk 181\n",
            "Processed chunk 182\n",
            "Processed chunk 183\n",
            "Processed chunk 184\n",
            "Processed chunk 185\n",
            "Processed chunk 186\n",
            "Processed chunk 187\n",
            "Processed chunk 188\n",
            "Processed chunk 189\n",
            "Processed chunk 190\n",
            "Processed chunk 191\n",
            "Processed chunk 192\n",
            "Processed chunk 193\n",
            "Processed chunk 194\n",
            "Processed chunk 195\n",
            "Processed chunk 196\n",
            "Processed chunk 197\n",
            "Processed chunk 198\n",
            "Processed chunk 199\n",
            "Processed chunk 200\n",
            "Processed chunk 201\n",
            "Processed chunk 202\n",
            "Processed chunk 203\n",
            "Processed chunk 204\n",
            "Processed chunk 205\n",
            "Processed chunk 206\n",
            "Processed chunk 207\n",
            "Processed chunk 208\n",
            "Processed chunk 209\n",
            "Processed chunk 210\n",
            "Processed chunk 211\n",
            "Processed chunk 212\n",
            "Processed chunk 213\n",
            "Processed chunk 214\n",
            "Processed chunk 215\n",
            "Processed chunk 216\n",
            "Processed chunk 217\n",
            "Processed chunk 218\n",
            "Processed chunk 219\n",
            "Processed chunk 220\n",
            "Processed chunk 221\n",
            "Processed chunk 222\n",
            "Processed chunk 223\n",
            "Processed chunk 224\n",
            "Processed chunk 225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Displaying first 100 rows***"
      ],
      "metadata": {
        "id": "vYzBKQ8UDe2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if input_data_df is not None:\n",
        "    # Display the first 100 rows of the DataFrame\n",
        "    print(input_data_df.head(100))\n",
        "else:\n",
        "    print(\"No valid data DataFrame generated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SaGj4XJCGhR",
        "outputId": "35951729-b11d-4aef-934d-a3677028a9aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ARTICLE_ID                 TITLE  \\\n",
            "0           0             Anarchism   \n",
            "1           0             Anarchism   \n",
            "2           0             Anarchism   \n",
            "3          21  Agricultural science   \n",
            "4          21  Agricultural science   \n",
            "..        ...                   ...   \n",
            "95         60          Alkali metal   \n",
            "96         61              Alphabet   \n",
            "97         61              Alphabet   \n",
            "98         61              Alphabet   \n",
            "99         61              Alphabet   \n",
            "\n",
            "                                    SECTION_TITLE  \\\n",
            "0                                    Introduction   \n",
            "1                       Etymology and terminology   \n",
            "2                                         History   \n",
            "3                                    Introduction   \n",
            "4   Agriculture agricultural science and agronomy   \n",
            "..                                            ...   \n",
            "95                                     References   \n",
            "96                                   Introduction   \n",
            "97                                      Etymology   \n",
            "98                                        History   \n",
            "99                                          Types   \n",
            "\n",
            "                                         SECTION_TEXT  \n",
            "0   '' 'anarch '' polit philosophi advoc self-gove...  \n",
            "1   term `` anarch '' compound word compos word ``...  \n",
            "2   ===origins=== woodcut digger document william ...  \n",
            "3   '' 'agricultur scienc '' broad multidisciplina...  \n",
            "4   three term often confus howev cover differ con...  \n",
            "..                                                ...  \n",
            "95                                                     \n",
            "96  edward bernard 's `` orbi erud '' compar known...  \n",
            "97  english word `` alphabet '' came middl english...  \n",
            "98  '' specimen '' typeset font languag william ca...  \n",
            "99  term `` alphabet '' use linguist paleograph wi...  \n",
            "\n",
            "[100 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pre-processing***"
      ],
      "metadata": {
        "id": "-q9WsXfBDkOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess text function\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Removing Punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Removing Stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Stemming or Lemmatization\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)  # Return preprocessed text as a single string"
      ],
      "metadata": {
        "id": "54NwxOaBCMB3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***File Writing***"
      ],
      "metadata": {
        "id": "VLpWrtXTEJAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_to_text_file(df, output_file):\n",
        "    with open(output_file, 'w') as f:\n",
        "        # Write data\n",
        "        for index, row in df.iterrows():\n",
        "            article_id = row['ARTICLE_ID']\n",
        "            section_text = row['SECTION_TEXT']\n",
        "\n",
        "            # Writing to the text file\n",
        "            f.write(f\"{article_id}','{section_text}\\n\")\n",
        "\n",
        "# Example usage:\n",
        "write_to_text_file(input_data_df, \"input.txt\")\n"
      ],
      "metadata": {
        "id": "hC7iRrPqOk9h"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}